{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of PULSE Demo",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xxxrokxxx/GDL/blob/master/Copy_of_PULSE_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1W8POs2cmUL",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Enter the number of images you want to generate, press the run button on the left and then while it is spinning, click on \"Choose Files\" to upload your own pictures\n",
        "number_of_images = 5#@param {type: \"number\"}\n",
        "show_intermediate_images = True#@param {type: \"boolean\"}\n",
        "downsampling_factor = \"32\"  #@param [32, 64]\n",
        "\n",
        "#@markdown <font color='red'>Many users have reported receiving an error message saying \"Google Drive Quota Exceeded.\" We are currently using Google Drive to store our model weights and it has a daily cap on downloads. If you are experiencing this error please try again later in the day or come back tomorrow. We apologize for the inconvenience.</font>\n",
        "#@markdown ### NOTE: PULSE only attempts to match the downscaled version of the image, and the output will likely not resemble the high resolution input image. ###\n",
        "\n",
        "#@markdown We suggest running this demo in Google Chrome. Using the show_intermediate_images option may slow down performance. PULSE works best on images where people are directly facing the camera. No data is stored. See: https://github.com/adamian98/pulse for additional information.\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "if not Path(\"PULSE.py\").exists():\n",
        "  if Path(\"pulse\").exists():\n",
        "    %cd pulse\n",
        "  else:\n",
        "    ! git clone https://github.com/adamian98/pulse\n",
        "    %cd pulse\n",
        "\n",
        "from google.colab import files\n",
        "from io import BytesIO\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "from PULSE import PULSE\n",
        "import torchvision\n",
        "from IPython import display\n",
        "import numpy as np\n",
        "from shape_predictor import align_face\n",
        "from drive import open_url\n",
        "import dlib\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "from bicubic import BicubicDownSample\n",
        "\n",
        "display.clear_output(wait=True)\n",
        "\n",
        "uploaded_names = files.upload().keys()\n",
        "\n",
        "if(len(uploaded_names)==0): raise Exception(\"You need to upload at least one image.\")\n",
        "\n",
        "f=open_url(\"https://drive.google.com/uc?id=1huhv8PYpNNKbGCLOaYUjOgR1pY5pmbJx\", cache_dir=\"cache\", return_path=True)\n",
        "predictor = dlib.shape_predictor(f)\n",
        "\n",
        "toPIL = torchvision.transforms.ToPILImage()\n",
        "toTensor = torchvision.transforms.ToTensor()\n",
        "D = BicubicDownSample(factor=int(downsampling_factor))\n",
        "\n",
        "images = []\n",
        "imagesHR = []\n",
        "for filename in uploaded_names:\n",
        "  for face in align_face(filename,predictor):\n",
        "    imagesHR.append(face)\n",
        "    face = toPIL(D(toTensor(face).unsqueeze(0).cuda()).cpu().detach().clamp(0,1)[0])\n",
        "    images.append(face)\n",
        "\n",
        "if(len(images)==0): raise Exception(\"No faces found. Try again with a different image.\")\n",
        "\n",
        "model = PULSE(cache_dir=\"cache\", verbose=False)\n",
        "\n",
        "kwargs={\n",
        " 'loss_str': '100*L2+0.05*GEOCROSS',\n",
        " 'seed': None,\n",
        " 'eps': 1e-3,\n",
        " 'noise_type': 'trainable',\n",
        " 'num_trainable_noise_layers': 5,\n",
        " 'tile_latent': False,\n",
        " 'bad_noise_layers': '17',\n",
        " 'opt_name': 'adam',\n",
        " 'learning_rate': 0.4,\n",
        " 'steps': 100,\n",
        " 'lr_schedule': 'linear1cycledrop',\n",
        " 'save_intermediate': True\n",
        "}\n",
        "dims = np.array((len(images),number_of_images+2))\n",
        "fig = plt.figure(figsize=20*dims)\n",
        "axs = ImageGrid(fig, 111, nrows_ncols=dims, axes_pad=0.2)\n",
        "\n",
        "im_downsample = Image.open(\"resources/downsample.png\")\n",
        "im_PULSE = Image.open(\"resources/PULSE.png\")\n",
        "\n",
        "display.clear_output(wait=True)\n",
        "image_list=[]\n",
        "for ax in axs:\n",
        "    image_list.append(ax.imshow(Image.new('RGB', (1024,1024), (255, 255, 255))))\n",
        "    ax.axis('off')\n",
        "\n",
        "for i,(imHR,imLR) in enumerate(zip(imagesHR,images)):\n",
        "    axs[i*dims[1]].imshow(imLR.resize((1024,1024),Image.NEAREST))\n",
        "    axs[i*dims[1]+1].imshow(im_PULSE)\n",
        "\n",
        "display.display(plt.gcf())\n",
        "display.clear_output(wait=True)\n",
        "\n",
        "for i,PIL_im in enumerate(images):\n",
        "    ref_im = torchvision.transforms.ToTensor()(PIL_im).unsqueeze(0).cuda()\n",
        "    for j in range(number_of_images):\n",
        "        running_text = axs[i*dims[1]+j+2].text(50,50,f\"Running...\",\n",
        "                                          {'family': 'serif','weight': 'normal','size': 12},\n",
        "                                          horizontalalignment='left',\n",
        "                                          verticalalignment='top',\n",
        "                                          bbox=dict(facecolor='white', alpha=1))\n",
        "        \n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)\n",
        "        for k,(HR,_) in enumerate(model(ref_im,**kwargs)):\n",
        "          if(show_intermediate_images):\n",
        "            if(k==0 or (k+1)%10==0):\n",
        "              PIL_out = toPIL(HR[0].cpu().detach().clamp(0, 1))\n",
        "              curr_image = image_list[i*dims[1]+j+2].set_data(PIL_out)\n",
        "              running_text.set_text(f\"Running ({k+1}%)\")\n",
        "              display.display(plt.gcf())\n",
        "              display.clear_output(wait=True)\n",
        "\n",
        "          if(k+1==kwargs[\"steps\"]):\n",
        "            PIL_out = toPIL(HR[0].cpu().detach().clamp(0, 1))\n",
        "            curr_image = image_list[i*dims[1]+j+2].set_data(PIL_out)\n",
        "            display.display(plt.gcf())\n",
        "            display.clear_output(wait=True)\n",
        "          \n",
        "        running_text.remove()\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "788PcSUM3Kvh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}